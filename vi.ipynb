{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Mapping, Tuple, TypeVar, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "MDPTransitions = Mapping[S, Mapping[A, Mapping[S, float]]]\n",
    "MDPActions = Mapping[S, Set[A]]\n",
    "MDPRewards = Mapping[S, Mapping[A, float]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_influence_tree(transitions) -> Mapping[S, Set[S]]:\n",
    "    '''\n",
    "    returns a mapping from state to all states that depend on that state in bellman equantions\n",
    "    '''\n",
    "    influence_tree = defaultdict(set)\n",
    "    for state in transitions:\n",
    "        for action in transitions[state]:\n",
    "            for next_state in transitions[state][action]:\n",
    "                influence_tree[next_state].add(state)\n",
    "    return influence_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(actions: MDPActions, transitions: MDPTransitions, rewards: MDPRewards,\n",
    "                   discount: float, vi_method: str='normal', k=None) -> Mapping[S, float]:\n",
    "    next_value_function = {s: 0 for s in actions.keys()}\n",
    "    base_value_function = None\n",
    "\n",
    "    num_iter = 0\n",
    "    if vi_method == 'normal':\n",
    "        while base_value_function is None or\\\n",
    "                not check_value_fuction_equivalence(base_value_function, next_value_function):\n",
    "            num_iter += 1\n",
    "            base_value_function = next_value_function\n",
    "            next_value_function = iterate_on_value_function(actions, transitions, rewards, base_value_function, \n",
    "                                                            discount)\n",
    "    elif vi_method == 'random-k':\n",
    "        while base_value_function is None or\\\n",
    "                not check_value_fuction_equivalence(base_value_function, next_value_function):\n",
    "            num_iter += 1\n",
    "            base_value_function = next_value_function\n",
    "\n",
    "            next_value_function = random_k_iterate_on_value_function(actions, transitions, rewards, base_value_function, \n",
    "                                                            discount, k)\n",
    "    elif vi_method == 'influence-tree':\n",
    "        influence_tree = get_influence_tree(transitions)\n",
    "        next_states_to_update = set(actions.keys())\n",
    "        while len(next_states_to_update) > 0:\n",
    "            num_iter += 1\n",
    "            base_value_function = next_value_function\n",
    "            next_value_function, updated_states = iterate_on_value_function_specific_states(actions, transitions,\n",
    "                                                                                            rewards,\n",
    "                                                                                            base_value_function, \n",
    "                                                            discount, next_states_to_update)\n",
    "            next_states_to_update = set()\n",
    "            for state in updated_states:\n",
    "                next_states_to_update.update(influence_tree[state])\n",
    "    elif vi_method == 'cyclic-vi':\n",
    "        while base_value_function is None or\\\n",
    "                not check_value_fuction_equivalence(base_value_function, next_value_function):\n",
    "            num_iter += 1\n",
    "            base_value_function = next_value_function\n",
    "            next_value_function = cycle_iterate_on_value_function(actions, transitions, rewards, \n",
    "                                                                  base_value_function, \n",
    "                                                                  discount)\n",
    "    elif vi_method == 'cyclic-vi-rp':\n",
    "        while base_value_function is None or\\\n",
    "                not check_value_fuction_equivalence(base_value_function, next_value_function):\n",
    "            num_iter += 1\n",
    "            base_value_function = next_value_function\n",
    "            next_value_function = cycle_iterate_on_value_function_rp(actions, transitions, rewards, \n",
    "                                                                  base_value_function, \n",
    "                                                                  discount)\n",
    "    else:\n",
    "        raise NotImplemented(f'have not implemented {vi_method} value iteration yet')\n",
    "    return base_value_function, num_iter\n",
    "\n",
    "\n",
    "def iterate_on_value_function(actions: MDPActions, transitions: MDPTransitions, rewards: MDPRewards,\n",
    "                              base_vf: Mapping[S, float], discount: float) -> Mapping[S, float]:\n",
    "    new_vf = {}\n",
    "    for s in actions.keys():\n",
    "        action_values = [(action, extract_value_of_action(actions, transitions, rewards, \n",
    "                                                          action, s, base_vf, discount)) for action in actions[s]]\n",
    "        best_action_reward = min([x[1] for x in action_values])\n",
    "        new_vf[s] = best_action_reward\n",
    "    return new_vf\n",
    "\n",
    "def cycle_iterate_on_value_function(actions: MDPActions, transitions: MDPTransitions, rewards: MDPRewards,\n",
    "                                    base_vf: Mapping[S, float], discount: float) -> Mapping[S, float]:\n",
    "    new_vf = base_vf.copy()\n",
    "    for s in actions.keys():\n",
    "        action_values = [(action, extract_value_of_action(actions, transitions, rewards, \n",
    "                                                          action, s, new_vf, discount)) for action in actions[s]]\n",
    "        best_action_reward = min([x[1] for x in action_values])\n",
    "        new_vf[s] = best_action_reward\n",
    "    return new_vf\n",
    "\n",
    "\n",
    "def cycle_iterate_on_value_function_rp(actions: MDPActions, transitions: MDPTransitions, rewards: MDPRewards,\n",
    "                                       base_vf: Mapping[S, float], discount: float) -> Mapping[S, float]:\n",
    "    new_vf = base_vf.copy()\n",
    "    states = list(actions.keys())\n",
    "    np.random.shuffle(states)\n",
    "    for s in states:\n",
    "        action_values = [(action, extract_value_of_action(actions, transitions, rewards, \n",
    "                                                          action, s, new_vf, discount)) for action in actions[s]]\n",
    "        best_action_reward = min([x[1] for x in action_values])\n",
    "        new_vf[s] = best_action_reward\n",
    "    return new_vf\n",
    "\n",
    "def random_k_iterate_on_value_function(actions: MDPActions, transitions: MDPTransitions, rewards: MDPRewards,\n",
    "                              base_vf: Mapping[S, float], discount: float, k: int) -> Mapping[S, float]:\n",
    "    new_vf = {}\n",
    "    states = list(actions.keys())\n",
    "    states_to_update_idx = np.random.choice(range(len(states)), size=k)\n",
    "    states_to_update = [states[idx] for idx in states_to_update_idx]\n",
    "    for s in states_to_update:\n",
    "        action_values = [(action, extract_value_of_action(actions, transitions, rewards, \n",
    "                                                          action, s, base_vf, discount)) for action in actions[s]]\n",
    "        best_action_reward = min([x[1] for x in action_values])\n",
    "        new_vf[s] = best_action_reward\n",
    "    for s in set(actions.keys()) - set(states_to_update):\n",
    "        new_vf[s] = base_vf[s]\n",
    "    return new_vf\n",
    "\n",
    "def iterate_on_value_function_specific_states(actions: MDPActions, transitions: MDPTransitions, rewards: MDPRewards,\n",
    "                                              base_vf: Mapping[S, float], discount: float, \n",
    "                                              states_to_update: Set[S]) -> Mapping[S, float]:\n",
    "    new_vf = {}\n",
    "    updated_states = set()\n",
    "    states = list(actions.keys())\n",
    "    for s in states_to_update:\n",
    "        action_values = [(action, extract_value_of_action(actions, transitions, rewards, \n",
    "                                                          action, s, base_vf, discount)) for action in actions[s]]\n",
    "        best_action_reward = min([x[1] for x in action_values])\n",
    "        new_vf[s] = best_action_reward\n",
    "        if abs(new_vf[s] - base_vf[s]) > 1e-8:\n",
    "            updated_states.add(s)\n",
    "    for s in set(actions.keys()) - set(states_to_update):\n",
    "        new_vf[s] = base_vf[s]\n",
    "    return new_vf, updated_states\n",
    "\n",
    "\n",
    "def extract_value_of_action(actions: MDPActions, transitions: MDPTransitions, rewards: MDPRewards,\n",
    "                            action: A, state: S, value_function, discount: float):\n",
    "    return rewards[state][action] + discount * sum([p * value_function[s_prime]\n",
    "                                                    for s_prime, p in\n",
    "                                                    transitions[state][action].items()])\n",
    "\n",
    "\n",
    "def check_value_fuction_equivalence(v1, v2, epsilon=1e-8) -> bool:\n",
    "    assert v1.keys() == v2.keys(), \"comparing policies with different state spaces\"\n",
    "    for state in v1:\n",
    "        if not abs(v1[state] - v2[state]) <= epsilon:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_policy_equivalence(p1, p2) -> bool:\n",
    "    assert p1.keys() == p2.keys(), \"comparing policies with different state spaces\"\n",
    "    for state in p1:\n",
    "        if p1[state] != p2[state]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_greedy_policy(actions: MDPActions, transitions: MDPTransitions, rewards: MDPRewards,\n",
    "                      value_function: Mapping[S, float], terminal_states: Set[S], \n",
    "                     discount: float) -> Mapping[S, A]:\n",
    "    policy = {}\n",
    "    non_terminal_states = set(actions.keys()) - terminal_states\n",
    "    for s in non_terminal_states:\n",
    "        actions_rewards = {}\n",
    "        for action in actions[s]:\n",
    "            actions_rewards[action] = extract_value_of_action(actions, transitions, rewards,\n",
    "                                                              action, s, value_function, discount)\n",
    "        policy[s] = {(min(actions_rewards, key=actions_rewards.get), 1)}\n",
    "    for s in terminal_states:\n",
    "        policy[s] = {(list(actions[s])[0], 1)}\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Maze Runner Problem\n",
    "'''\n",
    "\n",
    "maze_runner_actions = {\n",
    "    0: {'s', 'j'},\n",
    "    1: {'s', 'j'},\n",
    "    2: {'s', 'j'},\n",
    "    3: {'s', 'j'},\n",
    "    4: {'s',},\n",
    "    5: {'stay'}\n",
    "}\n",
    "\n",
    "maze_runner_transitions = {\n",
    "    0: {'s': {1: 1}, 'j': {2: 0.5, 3: 0.25, 4: 0.125, 5:0.125}},\n",
    "    1: {'s': {2: 1}, 'j': {3: 0.5, 4: 0.25, 5:0.25}},\n",
    "    2: {'s': {3: 1}, 'j': {4: 0.5, 5:0.5}},\n",
    "#     3: {'s': {4: 1}, 'j': {4: 0.5, 5:0.5}},\n",
    "    3: {'s': {4: 1}, 'j': {5:1}},\n",
    "    4: {'s': {5: 1}},\n",
    "    5: {'stay': {5: 1}}\n",
    "}\n",
    "\n",
    "maze_runner_rewards = {\n",
    "    0: {'s': 0, 'j': 0},\n",
    "    1: {'s': 0, 'j': 0},\n",
    "    2: {'s': 0, 'j': 0},\n",
    "    3: {'s': 0, 'j': 0},\n",
    "    4: {'s': 1},\n",
    "    5: {'stay': 0}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0, 5: 0.0} 2\n",
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0} 1\n",
      "{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0, 5: 0.0} 2\n",
      "{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0, 5: 0.0} 2\n",
      "{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0, 5: 0.0} 2\n"
     ]
    }
   ],
   "source": [
    "pure_vi, num_iter = value_iteration(actions=maze_runner_actions, transitions=maze_runner_transitions, rewards=maze_runner_rewards,\n",
    "               discount=0.9)\n",
    "print(pure_vi, num_iter)\n",
    "random_k_vi, num_iter_random_k = value_iteration(actions=maze_runner_actions, transitions=maze_runner_transitions, rewards=maze_runner_rewards,\n",
    "               discount=0.9, vi_method='random-k', k=3)\n",
    "print(random_k_vi, num_iter_random_k)\n",
    "\n",
    "tree_vi, num_iter_tree = value_iteration(actions=maze_runner_actions, transitions=maze_runner_transitions, rewards=maze_runner_rewards,\n",
    "               discount=0.9, vi_method='influence-tree')\n",
    "print(tree_vi, num_iter_tree)\n",
    "\n",
    "cyclic_vi, num_iter_cyclic = value_iteration(actions=maze_runner_actions, transitions=maze_runner_transitions, rewards=maze_runner_rewards,\n",
    "               discount=0.9, vi_method='cyclic-vi')\n",
    "print(cyclic_vi, num_iter_cyclic)\n",
    "\n",
    "rp_cyclic_vi, rp_num_iter_cyclic = value_iteration(actions=maze_runner_actions, transitions=maze_runner_transitions, rewards=maze_runner_rewards,\n",
    "               discount=0.9, vi_method='cyclic-vi-rp')\n",
    "print(rp_cyclic_vi, rp_num_iter_cyclic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maze(x_dim, y_dim, num_terminal, seed=None, deterministic=True):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    states = [(x,y) for x in range(x_dim) for y in range(y_dim)]\n",
    "    terminal_states_idx = np.random.choice(list(range(len(states))), size=num_terminal)\n",
    "    terminal_states = [states[idx] for idx in terminal_states_idx]\n",
    "    actions = {'l','r','u','d'}\n",
    "    state_transitions = {}\n",
    "    state_actions = {}\n",
    "    state_rewards = {}\n",
    "    for state in states:\n",
    "        state_transitions[state] = {}\n",
    "        state_rewards[state] = {}\n",
    "        if state in terminal_states:\n",
    "            state_actions[state] = {'stay'}\n",
    "        else:\n",
    "            state_actions[state] = actions\n",
    "        if deterministic:\n",
    "            for action in state_actions[state]:\n",
    "                next_coord = None\n",
    "                if action == 'stay':\n",
    "                    next_coord = state\n",
    "                elif action == 'l':\n",
    "                    next_coord = (state[0], max(0, state[1] - 1))\n",
    "                if action == 'r':\n",
    "                    next_coord = (state[0], min(x_dim-1, state[1] + 1))\n",
    "                if action == 'u':\n",
    "                    next_coord = (min(y_dim-1, state[0] + 1), state[1])\n",
    "                if action == 'd':\n",
    "                    next_coord = (max(0, state[0] - 1), state[1])\n",
    "                state_transitions[state][action] = {next_coord: 1}\n",
    "                if next_coord != state and next_coord in terminal_states:\n",
    "                    state_rewards[state][action] = np.random.choice([-1, 0, 1])\n",
    "                else:\n",
    "                    state_rewards[state][action] = 0\n",
    "        else:\n",
    "            raise NotImplemented('have not implemented non-deterministic transitions')\n",
    "    return state_actions, state_transitions, state_rewards, terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): {'u', 'r', 'l', 'd'}, (0, 1): {'u', 'r', 'l', 'd'}, (0, 2): {'u', 'r', 'l', 'd'}, (0, 3): {'u', 'r', 'l', 'd'}, (1, 0): {'u', 'r', 'l', 'd'}, (1, 1): {'u', 'r', 'l', 'd'}, (1, 2): {'u', 'r', 'l', 'd'}, (1, 3): {'u', 'r', 'l', 'd'}, (2, 0): {'u', 'r', 'l', 'd'}, (2, 1): {'u', 'r', 'l', 'd'}, (2, 2): {'u', 'r', 'l', 'd'}, (2, 3): {'u', 'r', 'l', 'd'}, (3, 0): {'stay'}, (3, 1): {'u', 'r', 'l', 'd'}, (3, 2): {'u', 'r', 'l', 'd'}, (3, 3): {'u', 'r', 'l', 'd'}}\n",
      "\n",
      "{(0, 0): {'u': {(1, 0): 1}, 'r': {(0, 1): 1}, 'l': {(0, 0): 1}, 'd': {(0, 0): 1}}, (0, 1): {'u': {(1, 1): 1}, 'r': {(0, 2): 1}, 'l': {(0, 0): 1}, 'd': {(0, 1): 1}}, (0, 2): {'u': {(1, 2): 1}, 'r': {(0, 3): 1}, 'l': {(0, 1): 1}, 'd': {(0, 2): 1}}, (0, 3): {'u': {(1, 3): 1}, 'r': {(0, 3): 1}, 'l': {(0, 2): 1}, 'd': {(0, 3): 1}}, (1, 0): {'u': {(2, 0): 1}, 'r': {(1, 1): 1}, 'l': {(1, 0): 1}, 'd': {(0, 0): 1}}, (1, 1): {'u': {(2, 1): 1}, 'r': {(1, 2): 1}, 'l': {(1, 0): 1}, 'd': {(0, 1): 1}}, (1, 2): {'u': {(2, 2): 1}, 'r': {(1, 3): 1}, 'l': {(1, 1): 1}, 'd': {(0, 2): 1}}, (1, 3): {'u': {(2, 3): 1}, 'r': {(1, 3): 1}, 'l': {(1, 2): 1}, 'd': {(0, 3): 1}}, (2, 0): {'u': {(3, 0): 1}, 'r': {(2, 1): 1}, 'l': {(2, 0): 1}, 'd': {(1, 0): 1}}, (2, 1): {'u': {(3, 1): 1}, 'r': {(2, 2): 1}, 'l': {(2, 0): 1}, 'd': {(1, 1): 1}}, (2, 2): {'u': {(3, 2): 1}, 'r': {(2, 3): 1}, 'l': {(2, 1): 1}, 'd': {(1, 2): 1}}, (2, 3): {'u': {(3, 3): 1}, 'r': {(2, 3): 1}, 'l': {(2, 2): 1}, 'd': {(1, 3): 1}}, (3, 0): {'stay': {(3, 0): 1}}, (3, 1): {'u': {(3, 1): 1}, 'r': {(3, 2): 1}, 'l': {(3, 0): 1}, 'd': {(2, 1): 1}}, (3, 2): {'u': {(3, 2): 1}, 'r': {(3, 3): 1}, 'l': {(3, 1): 1}, 'd': {(2, 2): 1}}, (3, 3): {'u': {(3, 3): 1}, 'r': {(3, 3): 1}, 'l': {(3, 2): 1}, 'd': {(2, 3): 1}}}\n",
      "\n",
      "{(0, 0): {'u': 0, 'r': 0, 'l': 0, 'd': 0}, (0, 1): {'u': 0, 'r': 0, 'l': 0, 'd': 0}, (0, 2): {'u': 0, 'r': 0, 'l': 0, 'd': 0}, (0, 3): {'u': 0, 'r': 0, 'l': 0, 'd': 0}, (1, 0): {'u': 0, 'r': 0, 'l': 0, 'd': 0}, (1, 1): {'u': 0, 'r': 0, 'l': 0, 'd': 0}, (1, 2): {'u': 0, 'r': 0, 'l': 0, 'd': 0}, (1, 3): {'u': 0, 'r': 0, 'l': 0, 'd': 0}, (2, 0): {'u': 0, 'r': 0, 'l': 0, 'd': 0}, (2, 1): {'u': 0, 'r': 0, 'l': 0, 'd': 0}, (2, 2): {'u': 0, 'r': 0, 'l': 0, 'd': 0}, (2, 3): {'u': 0, 'r': 0, 'l': 0, 'd': 0}, (3, 0): {'stay': 0}, (3, 1): {'u': 0, 'r': 0, 'l': -1, 'd': 0}, (3, 2): {'u': 0, 'r': 0, 'l': 0, 'd': 0}, (3, 3): {'u': 0, 'r': 0, 'l': 0, 'd': 0}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maze_actions, maze_transitions, maze_rewards, terminal_maze_state = get_maze(4,4, 1, seed=0)\n",
    "print(maze_actions)\n",
    "print()\n",
    "print(maze_transitions)\n",
    "print()\n",
    "print(maze_rewards)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): -0.6561000000000001, (0, 1): -0.7290000000000001, (0, 2): -0.6561000000000001, (0, 3): -0.5904900000000002, (1, 0): -0.7290000000000001, (1, 1): -0.81, (1, 2): -0.7290000000000001, (1, 3): -0.6561000000000001, (2, 0): -0.81, (2, 1): -0.9, (2, 2): -0.81, (2, 3): -0.7290000000000001, (3, 0): 0.0, (3, 1): -1.0, (3, 2): -0.9, (3, 3): -0.81}\n",
      "7\n",
      "{(3, 2): {('l', 1)}, (0, 0): {('u', 1)}, (1, 3): {('u', 1)}, (0, 2): {('u', 1)}, (2, 1): {('u', 1)}, (2, 3): {('u', 1)}, (1, 0): {('u', 1)}, (0, 3): {('u', 1)}, (0, 1): {('u', 1)}, (1, 2): {('u', 1)}, (3, 3): {('l', 1)}, (3, 1): {('l', 1)}, (2, 0): {('r', 1)}, (2, 2): {('u', 1)}, (1, 1): {('u', 1)}, (3, 0): {('stay', 1)}}\n",
      "\n",
      "{(0, 0): 0, (0, 1): 0, (0, 2): 0, (0, 3): 0, (1, 0): 0, (1, 1): 0, (1, 2): 0, (1, 3): 0, (2, 0): 0, (2, 1): 0, (2, 2): 0, (2, 3): 0, (3, 0): 0, (3, 1): 0, (3, 2): 0, (3, 3): 0}\n",
      "1\n",
      "{(3, 2): {('u', 1)}, (0, 0): {('u', 1)}, (1, 3): {('u', 1)}, (0, 2): {('u', 1)}, (2, 1): {('u', 1)}, (2, 3): {('u', 1)}, (1, 0): {('u', 1)}, (0, 3): {('u', 1)}, (0, 1): {('u', 1)}, (1, 2): {('u', 1)}, (3, 3): {('u', 1)}, (3, 1): {('l', 1)}, (2, 0): {('u', 1)}, (2, 2): {('u', 1)}, (1, 1): {('u', 1)}, (3, 0): {('stay', 1)}}\n",
      "\n",
      "{(1, 2): -0.7290000000000001, (0, 1): -0.7290000000000001, (1, 3): -0.6561000000000001, (0, 0): -0.6561000000000001, (2, 3): -0.7290000000000001, (1, 0): -0.7290000000000001, (0, 2): -0.6561000000000001, (0, 3): -0.5904900000000002, (3, 2): -0.9, (3, 3): -0.81, (3, 0): 0.0, (3, 1): -1.0, (2, 1): -0.9, (2, 0): -0.81, (2, 2): -0.81, (1, 1): -0.81}\n",
      "7\n",
      "{(3, 2): {('l', 1)}, (0, 0): {('u', 1)}, (1, 3): {('u', 1)}, (0, 2): {('u', 1)}, (2, 1): {('u', 1)}, (2, 3): {('u', 1)}, (1, 0): {('u', 1)}, (0, 3): {('u', 1)}, (0, 1): {('u', 1)}, (1, 2): {('u', 1)}, (3, 3): {('l', 1)}, (3, 1): {('l', 1)}, (2, 0): {('r', 1)}, (2, 2): {('u', 1)}, (1, 1): {('u', 1)}, (3, 0): {('stay', 1)}}\n",
      "\n",
      "{(0, 0): -0.6561000000000001, (0, 1): -0.7290000000000001, (0, 2): -0.6561000000000001, (0, 3): -0.5904900000000002, (1, 0): -0.7290000000000001, (1, 1): -0.81, (1, 2): -0.7290000000000001, (1, 3): -0.6561000000000001, (2, 0): -0.81, (2, 1): -0.9, (2, 2): -0.81, (2, 3): -0.7290000000000001, (3, 0): 0.0, (3, 1): -1.0, (3, 2): -0.9, (3, 3): -0.81}\n",
      "6\n",
      "{(3, 2): {('l', 1)}, (0, 0): {('u', 1)}, (1, 3): {('u', 1)}, (0, 2): {('u', 1)}, (2, 1): {('u', 1)}, (2, 3): {('u', 1)}, (1, 0): {('u', 1)}, (0, 3): {('u', 1)}, (0, 1): {('u', 1)}, (1, 2): {('u', 1)}, (3, 3): {('l', 1)}, (3, 1): {('l', 1)}, (2, 0): {('r', 1)}, (2, 2): {('u', 1)}, (1, 1): {('u', 1)}, (3, 0): {('stay', 1)}}\n",
      "\n",
      "{(0, 0): -0.6561000000000001, (0, 1): -0.7290000000000001, (0, 2): -0.6561000000000001, (0, 3): -0.5904900000000002, (1, 0): -0.7290000000000001, (1, 1): -0.81, (1, 2): -0.7290000000000001, (1, 3): -0.6561000000000001, (2, 0): -0.81, (2, 1): -0.9, (2, 2): -0.81, (2, 3): -0.7290000000000001, (3, 0): 0.0, (3, 1): -1.0, (3, 2): -0.9, (3, 3): -0.81}\n",
      "4\n",
      "{(3, 2): {('l', 1)}, (0, 0): {('u', 1)}, (1, 3): {('u', 1)}, (0, 2): {('u', 1)}, (2, 1): {('u', 1)}, (2, 3): {('u', 1)}, (1, 0): {('u', 1)}, (0, 3): {('u', 1)}, (0, 1): {('u', 1)}, (1, 2): {('u', 1)}, (3, 3): {('l', 1)}, (3, 1): {('l', 1)}, (2, 0): {('r', 1)}, (2, 2): {('u', 1)}, (1, 1): {('u', 1)}, (3, 0): {('stay', 1)}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pure_vi, num_iter = value_iteration(actions=maze_actions, transitions=maze_transitions, rewards=maze_rewards,\n",
    "               discount=0.9)\n",
    "print(pure_vi)\n",
    "print(num_iter)\n",
    "policy_p_vi = get_greedy_policy(value_function=pure_vi, actions=maze_actions, transitions=maze_transitions, rewards=maze_rewards,\n",
    "               terminal_states=set(terminal_maze_state), discount=0.9)\n",
    "print(policy_p_vi)\n",
    "print()\n",
    "\n",
    "\n",
    "k_vi, k_num_iter = value_iteration(actions=maze_actions, transitions=maze_transitions, rewards=maze_rewards,\n",
    "               discount=0.9, vi_method='random-k', k=5)\n",
    "\n",
    "print(k_vi)\n",
    "print(k_num_iter)\n",
    "policy_k_vi = get_greedy_policy(value_function=k_vi, actions=maze_actions, transitions=maze_transitions, rewards=maze_rewards,\n",
    "               terminal_states=set(terminal_maze_state), discount=0.9)\n",
    "print(policy_k_vi)\n",
    "print()\n",
    "\n",
    "\n",
    "tree_vi, tree_num_iter = value_iteration(actions=maze_actions, transitions=maze_transitions, rewards=maze_rewards,\n",
    "               discount=0.9, vi_method='influence-tree')\n",
    "\n",
    "print(tree_vi)\n",
    "print(tree_num_iter)\n",
    "policy_tree_vi = get_greedy_policy(value_function=tree_vi, actions=maze_actions, transitions=maze_transitions, rewards=maze_rewards,\n",
    "               terminal_states=set(terminal_maze_state), discount=0.9)\n",
    "print(policy_tree_vi)\n",
    "print()\n",
    "\n",
    "cyclic_vi, cyclic_num_iter = value_iteration(actions=maze_actions, transitions=maze_transitions, rewards=maze_rewards,\n",
    "               discount=0.9, vi_method='cyclic-vi')\n",
    "\n",
    "print(cyclic_vi)\n",
    "print(cyclic_num_iter)\n",
    "policy_cyclic_vi = get_greedy_policy(value_function=cyclic_vi, actions=maze_actions, transitions=maze_transitions, rewards=maze_rewards,\n",
    "               terminal_states=set(terminal_maze_state), discount=0.9)\n",
    "print(policy_cyclic_vi)\n",
    "print()\n",
    "\n",
    "rp_cyclic_vi, rp_cyclic_num_iter = value_iteration(actions=maze_actions, transitions=maze_transitions, rewards=maze_rewards,\n",
    "               discount=0.9, vi_method='cyclic-vi-rp')\n",
    "\n",
    "print(rp_cyclic_vi)\n",
    "print(rp_cyclic_num_iter)\n",
    "rp_policy_cyclic_vi = get_greedy_policy(value_function=rp_cyclic_vi, actions=maze_actions, transitions=maze_transitions, rewards=maze_rewards,\n",
    "               terminal_states=set(terminal_maze_state), discount=0.9)\n",
    "print(rp_policy_cyclic_vi)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
